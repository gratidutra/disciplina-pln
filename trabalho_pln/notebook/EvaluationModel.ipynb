{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84e7c734-5fe6-4f6c-8b11-fc0b0ea92f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d192ea8d-5c11-4647-91dd-69d9e90411bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['annotator', 'pmid', 'Gene', 'Measure', 'Method', 'Mutation'], dtype='object')\n",
      "Index(['annotator', 'text', 'Gene', 'Measure', 'Method', 'Mutation'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_model = pd.read_csv('../data/result_model.csv')\n",
    "\n",
    "df_model = df_model.rename(columns={\"GENE_OR_GENE_PRODUCT\": \"Gene\", \"MEASURE\": \"Measure\",\n",
    "                                   \"METHOD\": \"Method\", \"MUTATION\": \"Mutation\"})\n",
    "df_model['annotator'] = 'model'\n",
    "\n",
    "df_model=df_model[['annotator', 'pmid', 'Gene', 'Measure', 'Method', 'Mutation']]\n",
    "\n",
    "print(df_model.columns)\n",
    "\n",
    "df_double_checkers = pd.read_csv('../data/double_checkers.csv')\n",
    "\n",
    "df_double_checker = df_double_checkers[['annotator', 'text', 'Gene', 'Measure', 'Method', 'Mutation',]]\n",
    "\n",
    "print(df_double_checker.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b900dd7-ba4e-41a7-a752-a3cf12d26293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entities(entity_str):\n",
    "    if pd.isna(entity_str):\n",
    "        return set()\n",
    "    return set(e.strip().lower() for e in entity_str.split(\",\") if e.strip())\n",
    "\n",
    "def flatten_annotations(df, cols):\n",
    "    all_entities = {col: [] for col in cols}\n",
    "    for col in cols:\n",
    "        for ents in df[col].fillna(\"\"):\n",
    "            all_entities[col].extend([e.strip().lower() for e in ents.split(\",\") if e.strip()])\n",
    "        all_entities[col] = set(all_entities[col])\n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb3050d9-7381-4ede-bbb6-f6bb14dcdb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas a comparar\n",
    "entity_cols = [\"Mutation\", \"Gene\", \"Measure\", \"Method\"]\n",
    "\n",
    "# Agrupa todos os checadores (exceto 'model')\n",
    "#df_checkers = df[df[\"annotator\"] != \"model\"]\n",
    "gold_entities = flatten_annotations(df_double_checkers, entity_cols)\n",
    "\n",
    "# Agrupa o modelo\n",
    "#df_model = df[df[\"annotator\"] == \"model\"]\n",
    "model_entities = flatten_annotations(df_model, entity_cols)\n",
    "\n",
    "# Calcular métricas por categoria e geral\n",
    "results = []\n",
    "for col in entity_cols:\n",
    "    y_true = [1 if e in gold_entities[col] else 0 for e in model_entities[col]]\n",
    "    y_pred = [1]*len(model_entities[col])  # tudo que o modelo previu\n",
    "    # Para recall, adicionamos os FNs (que estão no gold mas não no modelo)\n",
    "    missing = [e for e in gold_entities[col] if e not in model_entities[col]]\n",
    "    y_true.extend([1]*len(missing))\n",
    "    y_pred.extend([0]*len(missing))\n",
    "\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    results.append((col, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42da5066-a1c6-4add-baf1-99ecf76f077e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Entity  Precision    Recall        F1\n",
      "0        Mutation   0.144578  0.240000  0.180451\n",
      "1            Gene   0.235294  0.300000  0.263736\n",
      "2         Measure   1.000000  0.066667  0.125000\n",
      "3          Method   1.000000  0.044444  0.085106\n",
      "Overall       NaN   0.594968  0.162778  0.163573\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Entity\", \"Precision\", \"Recall\", \"F1\"])\n",
    "df_results.loc[\"Overall\"] = df_results[[\"Precision\", \"Recall\", \"F1\"]].mean()\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
