{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JLGC9t_7Upm"
   },
   "source": [
    "# Proposta de Atividade de Aula 30/06\n",
    "## Recuperação de Informação em Sistemas Chatbot com RAG\n",
    "### Grupo 1: Marilia e Murilo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwcvG6qojbtH"
   },
   "source": [
    "\n",
    "\n",
    "### Atividade 1:\n",
    "**Objetivo:** Transformar textos em embeddings, simulando um armazenamento vetorizado para posterior busca\n",
    "\n",
    "**Contexto:** Essa etapa simula a fase de Indexing, onde  documentos externos são processados, divididos em \"chunks\" (pedaços) e transformados em vetores numéricos (embeddings). Você pode eleger o critério para separação dos chuncks, por exemplo, parágrafos, frases, limite de tokens, etc.\n",
    "\n",
    "**Passos da Atividade:**\n",
    "\n",
    "a) Coletar o texto da Página https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap16/cap16.html e armazenar em um documento de texto.\n",
    "\n",
    "b) Salvar chuncks em um arquivo csv com a seguinte estrutura:\n",
    "\n",
    "| id  | nome\\_arquivo       | texto\\_artigo | vector (lista)    | numero\\_tokens |\n",
    "| --- | ------------------- | ------------- | ----------------- | -------------- |\n",
    "| 1   | brasileiraspln.docx | \"Texto 1...\"  | \\[0.34, ..., ...] | 98             |\n",
    "| 2   | brasileiraspln.docx | \"Texto 2...\"  | \\[0.22, ..., ...] | 97             |\n",
    "| ... | ...                 | ...           | ...               | ...            |\n",
    "\n",
    "Faça esse processo para pelo menos 2 modelos de embedding.\n",
    "- all-MiniLM-L6-v2\n",
    "- paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grati/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Baixar tokenizer do NLTK\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qRyD1Tmt7WTX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo gerado: chunks_35palavras_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "#all-MiniLM-L6-v2\n",
    "#paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "doc_path = \"brasileiraspln_cap16.docx\"\n",
    "doc = Document(doc_path)\n",
    "text = \" \".join([p.text for p in doc.paragraphs if p.text.strip() != \"\"])\n",
    "\n",
    "text = re.sub(r'\\s+', ' ', text) \n",
    "words = word_tokenize(text, language=\"portuguese\")\n",
    "\n",
    "chunk_size = 35\n",
    "chunks = []\n",
    "for i in range(0, len(words), chunk_size):\n",
    "    chunk = words[i:i+chunk_size]\n",
    "    chunk_text = ' '.join(chunk)\n",
    "    chunks.append(chunk_text)\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "vectors = model.encode(chunks)\n",
    "\n",
    "data = []\n",
    "for i, (chunk, vec) in enumerate(zip(chunks, vectors), start=1):\n",
    "    data.append({\n",
    "        \"id\": i,\n",
    "        \"nome_arquivo\": \"brasileiraspln.docx\",\n",
    "        \"texto_artigo\": chunk,\n",
    "        \"vector\": vec.tolist(),  \n",
    "        \"numero_tokens\": len(word_tokenize(chunk, language=\"portuguese\"))\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(\"chunks_35palavras_embeddings.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Arquivo gerado: chunks_35palavras_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FecPfO9z7Wjf"
   },
   "source": [
    "### Atividade 2\n",
    "**Objetivo:** Avaliar e comparar a eficácia de três métodos de recuperação de documentos para uma consulta específica.\n",
    "\n",
    "**Contexto:** Usaremos um pequeno conjunto de documentos (2 arquivos csv) e uma pergunta.\n",
    "\n",
    "**Métodos a Comparar:**\n",
    "* Baseline (Busca Esparsa): BM25\n",
    "* Busca Densa 1 (Multilíngue): Embedding com paraphrase-multilingual-MiniLM-L12-v2\n",
    "* Busca Densa 2 (Multilíngue): Embedding com all-MiniLM-L6-v2\n",
    "\n",
    "**Passos da Atividade:**\n",
    "\n",
    "a) Para a mesma pergunta, obter a lista ordenada de documentos de cada um dos três métodos.\n",
    "\n",
    "b) Calcular as seguintes métricas de avaliação para cada método:\n",
    "- MRR (Mean Reciprocal Rank): Mede a posição do primeiro resultado relevante.\n",
    "- NDCG@10 (Normalized Discounted Cumulative Gain): Mede a qualidade do ranking dos 10 primeiros resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYMWwb248kkk"
   },
   "source": [
    "FUNÇÕES DE AVALIAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ijjfYDnr8K0c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ast\n",
    "\n",
    "# ======= Funções de avaliação =======\n",
    "\n",
    "def compute_mrr(ranked_ids, relevant_ids):\n",
    "    for rank, doc_id in enumerate(ranked_ids, start=1):\n",
    "        if doc_id in relevant_ids:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_ndcg(ranked_ids, relevant_ids, k=10):\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(ranked_ids[:k]):\n",
    "        if doc_id in relevant_ids:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_ids), k)))\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "# ======= Similaridade de cosseno =======\n",
    "\n",
    "def cosine_rank(df, question_embedding):\n",
    "    matrix = np.vstack(df['vector'].values)\n",
    "    sims = cosine_similarity([question_embedding], matrix)[0]\n",
    "    df['score'] = sims\n",
    "    return df.sort_values('score', ascending=False)['id'].tolist()\n",
    "\n",
    "# ======= BM25 =======\n",
    "\n",
    "def bm25_rank(df_bm25, question):\n",
    "    tokenized_corpus = [str(doc).split() for doc in df_bm25['texto_artigo']]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    tokenized_question = question.split()\n",
    "    scores = bm25.get_scores(tokenized_question)\n",
    "    df_bm25['score'] = scores\n",
    "    return df_bm25.sort_values('score', ascending=False)['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_model_1 = pd.read_csv('chunks_35palavras_embeddings2.csv')\n",
    "embedings_model = pd.read_csv('chunks_35palavras_embeddings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUSCA ESPARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Avaliação da Pergunta =====\n",
      "Pergunta: O que é modelo booleano?\n",
      "\n",
      "-- BM25 --\n",
      "Ranking: [85, 142, 138, 150, 143, 140, 139, 167, 91, 164]\n",
      "MRR: 1.0\n",
      "NDCG@10: 0.3576223542196109\n",
      "\n",
      "-- Embedding: paraphrase-multilingual-MiniLM-L12-v2 --\n",
      "Ranking: [85, 91, 150, 142, 82, 83, 60, 129, 139, 160]\n",
      "MRR: 1.0\n",
      "NDCG@10: 0.4483039899025303\n",
      "\n",
      "-- Embedding: all-MiniLM-L6-v2 --\n",
      "Ranking: [85, 91, 90, 86, 96, 82, 69, 150, 142, 97]\n",
      "MRR: 1.0\n",
      "NDCG@10: 0.7041249493150387\n"
     ]
    }
   ],
   "source": [
    "df_dense1 = pd.read_csv(\"chunks_35palavras_embeddings.csv\")   # paraphrase-multilingual-MiniLM-L12-v2\n",
    "df_dense2 = pd.read_csv(\"chunks_35palavras_embeddings2.csv\")      # all-MiniLM-L6-v2\n",
    "df_bm25 = pd.read_csv(\"chunks_35palavras_embeddings.csv\")[['id', 'texto_artigo', 'numero_tokens']]     # Pode ser o mesmo texto, sem importar os vetores\n",
    "\n",
    "df_dense1['vector'] = df_dense1['vector'].apply(ast.literal_eval)\n",
    "df_dense2['vector'] = df_dense2['vector'].apply(ast.literal_eval)\n",
    "\n",
    "df_dense1['vector'] = df_dense1['vector'].apply(np.array)\n",
    "df_dense2['vector'] = df_dense2['vector'].apply(np.array)\n",
    "\n",
    "question = \"O que é modelo booleano?\"  # exemplo\n",
    "relevant_ids = [85,86,87,88,89, 90,91]  \n",
    "\n",
    "model_1 = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "model_2 = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embedding_q1 = model_1.encode(question)\n",
    "embedding_q2 = model_2.encode(question)\n",
    "\n",
    "# ======= Rankear documentos =======\n",
    "rank_bm25 = bm25_rank(df_bm25.copy(), question)\n",
    "rank_dense1 = cosine_rank(df_dense1.copy(), embedding_q1)\n",
    "rank_dense2 = cosine_rank(df_dense2.copy(), embedding_q2)\n",
    "\n",
    "# ======= Avaliar =======\n",
    "print(\"===== Avaliação da Pergunta =====\")\n",
    "print(\"Pergunta:\", question)\n",
    "print(\"\\n-- BM25 --\")\n",
    "print(\"Ranking:\", rank_bm25[:10])\n",
    "print(\"MRR:\", compute_mrr(rank_bm25, relevant_ids))\n",
    "print(\"NDCG@10:\", compute_ndcg(rank_bm25, relevant_ids))\n",
    "\n",
    "print(\"\\n-- Embedding: paraphrase-multilingual-MiniLM-L12-v2 --\")\n",
    "print(\"Ranking:\", rank_dense1[:10])\n",
    "print(\"MRR:\", compute_mrr(rank_dense1, relevant_ids))\n",
    "print(\"NDCG@10:\", compute_ndcg(rank_dense1, relevant_ids))\n",
    "\n",
    "print(\"\\n-- Embedding: all-MiniLM-L6-v2 --\")\n",
    "print(\"Ranking:\", rank_dense2[:10])\n",
    "print(\"MRR:\", compute_mrr(rank_dense2, relevant_ids))\n",
    "print(\"NDCG@10:\", compute_ndcg(rank_dense2, relevant_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
