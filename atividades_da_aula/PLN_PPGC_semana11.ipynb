{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnjPYs-RuMyc"
   },
   "source": [
    "# Representação de textos: Embeddings (Word2Vec)\n",
    "\n",
    "*   Sklearn -> https://www.youtube.com/watch?v=bwqBZ4IuX7Q\n",
    "*   Pratical NLP Book -> https://github.com/practical-nlp/practical-nlp-code/tree/master/Ch3\n",
    "* Vídeo: https://www.youtube.com/watch?v=Do8cVbx-HOs&list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX&index=19\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ohf---Mx96zM",
    "outputId": "99709cfe-2294-4c38-92ec-f06462944379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsZ3m_BRr2ep",
    "outputId": "4ba20657-78c9-4480-9e88-ea77f0bc8257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/grati/miniconda3/envs/drugdiscovery/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/grati/miniconda3/envs/drugdiscovery/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/grati/miniconda3/envs/drugdiscovery/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m331.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, gensim\n",
      "\u001b[2K  Attempting uninstall: scipy\n",
      "\u001b[2K    Found existing installation: scipy 1.15.2\n",
      "\u001b[2K    Uninstalling scipy-1.15.2:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.2━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [gensim]━━━━\u001b[0m \u001b[32m1/2\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.3.3 scipy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP4w1br4hEgK"
   },
   "source": [
    "### Exercícios de aplicação com Word2Vec (inglês)\n",
    "\n",
    "\n",
    "Link: https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08\n",
    "\n",
    "Dados: https://www.kaggle.com/datasets/anu0012/hotel-review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcPKhTZCKTRM",
    "outputId": "17d3292d-7f48-44e1-e38c-2d94cb8bd11d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/grati/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('cbow_s50.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Pim_aaachDye",
    "outputId": "d8907295-748e-4dbf-dd2b-9393ce58cecd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "csv_path = \"data/train.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, on_bad_lines='warn')\n",
    "\n",
    "def clean_data(text):\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿ/]+', '', text)\n",
    "    text = re.sub(r'[\\\\/×\\^\\]\\[÷]', '', text)\n",
    "    return text\n",
    "\n",
    "def change_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "def remover(text):\n",
    "    text_tokens = text.split(\" \")\n",
    "    final_list = [word for word in text_tokens if not word in stopwords_list]\n",
    "    text = ' '.join(final_list)\n",
    "    return text\n",
    "\n",
    "def get_w2vdf(df):\n",
    "    w2v_df = pd.DataFrame(df[\"Description\"]).values.tolist()\n",
    "    for i in range(len(w2v_df)):\n",
    "        w2v_df[i] = w2v_df[i][0].split(\" \")\n",
    "    return w2v_df\n",
    "\n",
    "def train_w2v(w2v_df):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(min_count=4,\n",
    "                         window=4,\n",
    "                         vector_size=300,\n",
    "                         alpha=0.03,\n",
    "                         min_alpha=0.0007,\n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    w2v_model.build_vocab(w2v_df, progress_per=10000)\n",
    "    w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72D8ZhF0KcJb"
   },
   "outputs": [],
   "source": [
    "# df[[\"reviews.text\"]] = df[[\"reviews.text\"]].astype(str)\n",
    "# df[\"reviews.text\"] = df[\"reviews.text\"].apply(change_lower)\n",
    "# df[\"reviews.text\"] = df[\"reviews.text\"].apply(clean_data)\n",
    "# df[\"reviews.text\"] = df[\"reviews.text\"].apply(remover)\n",
    "\n",
    "df[[\"Description\"]] = df[[\"Description\"]].astype(str)\n",
    "df[\"Description\"] = df[\"Description\"].apply(change_lower)\n",
    "df[\"Description\"] = df[\"Description\"].apply(clean_data)\n",
    "df[\"Description\"] = df[\"Description\"].apply(remover)\n",
    "\n",
    "w2v_df = get_w2vdf(df)\n",
    "w2v_model = train_w2v(w2v_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvkVo1gE-ojl"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-7Aair3-pgy"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"terrible\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0utAvlptjUu3"
   },
   "source": [
    "### Exercícios de aplicação com Word2Vec (espanhol)\n",
    "Link: https://keepcoding.io/blog/ejercicio-de-aplicacion-con-word2vec/\n",
    "\n",
    "Dados: https://www.kaggle.com/datasets/kevinmorgado/spanish-news-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZC7yPhwkTnf-"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShlWNTh1jxFZ"
   },
   "outputs": [],
   "source": [
    "corpus = LineSentence('spanish-news.csv', limit = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTPlN-b3j1pJ"
   },
   "outputs": [],
   "source": [
    "sg_params = {\n",
    "          'vector_size' : 20,\n",
    "          'window' : 10,\n",
    "          'min_count' : 5,\n",
    "          'sg' : 1,\n",
    "          'hs' : 0,\n",
    "          'negative' : 20,\n",
    "}\n",
    "\n",
    "cbow_params = {\n",
    "          'vector_size' : 20,\n",
    "          'window' : 10,\n",
    "          'min_count' : 5,\n",
    "          'sg' : 0,\n",
    "          'hs' : 0,\n",
    "          'negative' : 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laIzl3xXj4yW"
   },
   "outputs": [],
   "source": [
    "#Skip Gram\n",
    "w2v_sg = Word2Vec(** sg_params)\n",
    "\n",
    "#CBOW\n",
    "w2v_cbow = Word2Vec(** cbow_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfhQp6ktj7tA"
   },
   "outputs": [],
   "source": [
    "#Skip Gram\n",
    "w2v_sg.build_vocab(corpus)\n",
    "\n",
    "#CBOW\n",
    "w2v_cbow.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IuQ3Ve7kQjz",
    "outputId": "07615cf3-e43e-4176-9b6b-b02464738065"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685700, 1306180)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skip Gram\n",
    "w2v_sg.train(corpus_iterable = corpus, total_examples = w2v_sg.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh9fgaVRkTPS",
    "outputId": "ffbc39f5-e352-4a57-c86e-83f06da25d67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(686208, 1306180)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CBOW\n",
    "w2v_cbow.train(corpus_iterable = corpus, total_examples = w2v_cbow.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo-y1iCPkWgq"
   },
   "outputs": [],
   "source": [
    "w2v_sg.save('./w2v_sg_ d300_mc5_ w10.pkl')\n",
    "w2v_cbow.save('./w2v_sg_ d300_mc5_ w10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PS7ASkPc9dgt"
   },
   "outputs": [],
   "source": [
    "def print_sim_words(word, model_cbow, model_sg):\n",
    "    \"\"\"Prints similar words for a given word using both CBOW and Skip-Gram models.\"\"\"\n",
    "\n",
    "    print(f\"Similar words to '{word}' (CBOW):\")\n",
    "    for similar_word, similarity in model_cbow.wv.most_similar(word):\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    print(f\"\\nSimilar words to '{word}' (Skip-Gram):\")\n",
    "    for similar_word, similarity in model_sg.wv.most_similar(word):\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZTBUJDJkYyr",
    "outputId": "b3db16e7-de6a-419b-ee26-3dd3aada44b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'elecciones' (CBOW):\n",
      "  fuera: 0.9963\n",
      "  crecieron: 0.9962\n",
      "  doble: 0.9955\n",
      "  tuvieron: 0.9952\n",
      "  estadística: 0.9952\n",
      "  aumentando: 0.9952\n",
      "  Turquía: 0.9951\n",
      "  prevén: 0.9950\n",
      "  FOMC: 0.9949\n",
      "  Central: 0.9948\n",
      "\n",
      "Similar words to 'elecciones' (Skip-Gram):\n",
      "  estadounidense: 0.9692\n",
      "  ajustes: 0.9684\n",
      "  virus: 0.9617\n",
      "  semanas: 0.9582\n",
      "  precios.: 0.9498\n",
      "  pronósticos: 0.9497\n",
      "  probablemente: 0.9465\n",
      "  causas: 0.9447\n",
      "  recesión: 0.9421\n",
      "  interno: 0.9414\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_sim_words('elecciones', w2v_cbow, w2v_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-gqdFFaLJwU"
   },
   "source": [
    "### Comparando Word2Vec, Glove e FastText\n",
    "Link: https://medium.com/@mervebdurna/advanced-word-embeddings-word2vec-glove-and-fasttext-26e546ffedbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "in8czdHhQEYC"
   },
   "outputs": [],
   "source": [
    "!pip install glove-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzECvU0sTAIn",
    "outputId": "db82e341-ffa4-4847-bf62-f6d26b4912aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YkQfRQ8LIas",
    "outputId": "23b2239e-061e-4c32-ee2f-84eb73e8b95c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n"
     ]
    }
   ],
   "source": [
    "# Code Example with Toy Dataset\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Toy dataset\n",
    "sentences = [\"I love natural language processing.\",\n",
    "             \"Word embeddings are powerful.\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access embeddings\n",
    "word_embeddings = model.wv\n",
    "print(word_embeddings['natural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uk4PtV4lTJRt",
    "outputId": "1e4672a2-d204-4990-b9ba-8818afca99ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Embedding for 'glove': [-1.10675337e-03 -3.24429905e-03 -3.58508580e-03 -4.14058402e-03\n",
      " -2.01957304e-03 -3.33836085e-03  2.13817485e-03 -5.78321509e-04\n",
      "  2.02532813e-03  6.04360047e-04  3.77675097e-03 -2.78383440e-04\n",
      " -4.77656701e-04  2.45030037e-03  3.45739369e-03 -2.88659816e-03\n",
      "  4.40631336e-03 -6.03074771e-04  3.33246233e-03  3.26901194e-03\n",
      "  2.88213702e-03  4.28857362e-03  1.79638300e-03  3.79029344e-03\n",
      "  5.02457618e-03 -4.15062544e-03 -3.28398403e-03 -4.36928350e-03\n",
      " -3.89783777e-03 -1.04070763e-03 -6.67592954e-04  2.89479569e-03\n",
      "  2.25368724e-04  4.16515835e-03  4.72576193e-03 -3.65519017e-04\n",
      " -4.69906826e-03  4.83716493e-04 -4.08917522e-03 -3.49092671e-03\n",
      "  2.62931448e-03 -4.81431568e-03  3.00404045e-03 -3.14439837e-03\n",
      "  3.65050260e-03 -3.44148661e-03 -4.54839780e-03  4.83601949e-03\n",
      " -3.41755985e-03 -1.26091804e-03 -2.09862423e-05 -3.54595074e-03\n",
      " -7.96365587e-04 -4.06944044e-03  4.11373820e-03  2.38406655e-03\n",
      "  1.40308449e-03  7.62125658e-04  1.23296611e-03  4.07308971e-03\n",
      " -4.39982484e-03 -2.31486178e-03 -1.30284983e-03 -4.96913405e-04\n",
      "  1.74897048e-03  2.83383591e-03 -8.44184083e-04  1.87494382e-03\n",
      "  4.22127250e-04 -3.49197733e-03  3.32322310e-03  2.73151379e-03\n",
      "  2.10913301e-03 -2.83338118e-04  1.65283296e-04  2.82516468e-03\n",
      "  3.30249435e-03 -3.84481558e-03 -7.52600790e-05  3.62460342e-03\n",
      " -1.66256861e-03  1.12480034e-03 -3.07026046e-03 -1.49871062e-03\n",
      " -1.27647948e-03  1.31113783e-03 -4.87476552e-03 -1.08184725e-03\n",
      "  1.62622967e-03  1.10320241e-03 -2.86196108e-03  4.93843213e-03\n",
      " -2.42860973e-03  2.90539427e-03  2.84963852e-06 -3.93713322e-04\n",
      "  4.27444785e-03 -1.26285483e-03 -4.13640860e-03 -9.95725398e-04]\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Toy dataset\n",
    "sentences = [\"Word embeddings capture semantic meanings.\",\n",
    "             \"GloVe is an impactful word embedding model.\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Creating a corpus object\n",
    "corpus = Corpus()\n",
    "\n",
    "# Training the corpus to generate the co-occurrence matrix\n",
    "corpus.fit(tokenized_sentences, window=10)\n",
    "\n",
    "# Training the GloVe model\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "# Retrieve and display word embeddings\n",
    "word = \"glove\"\n",
    "embedding = glove.word_vectors[glove.dictionary[word]]\n",
    "print(f\"Embedding for '{word}': {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5K7CvSUSTQyU",
    "outputId": "9dd4ac6a-d39f-4610-8c71-4ae00815a326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2919701e-03 -1.0602611e-04 -1.1323356e-03  1.6584302e-03\n",
      " -5.7117449e-04 -2.9840841e-04 -4.3193492e-04 -3.1250282e-04\n",
      " -1.9898350e-04  9.4852143e-04  1.6994212e-03 -1.8581563e-04\n",
      " -8.1228669e-04 -1.5968895e-03 -1.3839703e-03 -6.3576088e-05\n",
      " -5.7436171e-04 -1.1720147e-03  7.4763177e-04 -2.1684753e-05\n",
      "  4.5981101e-04 -1.7291495e-03 -4.4365969e-04  4.0478929e-04\n",
      "  1.2072949e-04  6.4071972e-04 -1.0785459e-03  1.3050955e-03\n",
      "  3.5044085e-04 -1.8284899e-04 -1.5951110e-04 -1.0465594e-03\n",
      " -1.5170674e-04 -5.7858619e-04 -1.8307484e-03  1.0248278e-03\n",
      "  6.9344341e-04  1.6159177e-03 -8.4400486e-04  8.9535897e-04\n",
      " -1.3508157e-04  2.3538095e-03 -3.7109022e-04 -2.8064058e-04\n",
      "  2.6269807e-04 -2.8326022e-04 -7.7332847e-04  1.8949938e-03\n",
      "  2.1798143e-03 -4.4569728e-04 -6.4175081e-04  1.4240020e-04\n",
      "  2.5182988e-03 -1.5666584e-03  1.3954224e-04 -6.9046958e-04\n",
      "  5.8793183e-04 -1.4282564e-03  2.1278318e-04 -2.2993281e-03\n",
      " -4.3249400e-03 -1.6397990e-03  1.3989839e-03 -1.3229308e-03\n",
      "  2.0258871e-03 -2.9663873e-04 -1.7821314e-03 -1.9624084e-04\n",
      " -3.8101443e-04  1.2712786e-03 -4.8752314e-05 -1.6701949e-03\n",
      " -4.0860524e-04 -2.3929863e-03 -1.1424356e-03 -8.1383681e-04\n",
      " -4.9799628e-04 -1.4722025e-03 -1.8545202e-03 -4.9134775e-04\n",
      "  2.1096619e-03 -3.5232501e-05  3.5016984e-03  1.4268994e-04\n",
      "  7.3083641e-04 -1.6456110e-03 -1.8567833e-03 -8.2786253e-04\n",
      "  2.2351660e-04  2.1772250e-03  1.3073307e-03  7.8967125e-05\n",
      "  2.6948133e-04 -2.0950858e-03  2.1080931e-03  4.0393372e-04\n",
      "  1.4223440e-03  6.8460318e-04  2.4361876e-03  5.3560670e-04]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Toy dataset\n",
    "sentences = [\"FastText embeddings handle subword information.\",\n",
    "             \"It is effective for various languages.\"]\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access embeddings\n",
    "word_embeddings = model.wv\n",
    "print(word_embeddings['subword'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
